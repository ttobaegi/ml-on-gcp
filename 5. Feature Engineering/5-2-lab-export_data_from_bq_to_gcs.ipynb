{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting data from BigQuery to Google Cloud Storage\n",
    "\n",
    "In this notebook, we export BigQuery data to GCS so that we can reuse our Keras model that was developed on CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chown command to change the ownership of the repository\n",
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery==1.25.0\n",
      "  Downloading google_cloud_bigquery-1.25.0-py2.py3-none-any.whl (169 kB)\n",
      "     |████████████████████████████████| 169 kB 8.7 MB/s            \n",
      "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.1.0\n",
      "  Downloading google_cloud_core-1.7.2-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (3.19.1)\n",
      "Collecting google-resumable-media<0.6dev,>=0.5.0\n",
      "  Downloading google_resumable_media-0.5.1-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: six<2.0.0dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.16.0)\n",
      "Collecting google-auth<2.0dev,>=1.9.0\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     |████████████████████████████████| 152 kB 62.0 MB/s            \n",
      "\u001b[?25hCollecting google-api-core<2.0dev,>=1.15.0\n",
      "  Downloading google_api_core-1.31.5-py2.py3-none-any.whl (93 kB)\n",
      "     |████████████████████████████████| 93 kB 2.6 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (2.26.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (59.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (1.53.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (2021.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (0.2.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (3.0.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (3.1)\n",
      "Installing collected packages: google-auth, google-api-core, google-resumable-media, google-cloud-core, google-cloud-bigquery\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.3.3\n",
      "    Uninstalling google-auth-2.3.3:\n",
      "      Successfully uninstalled google-auth-2.3.3\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 2.3.2\n",
      "    Uninstalling google-api-core-2.3.2:\n",
      "      Successfully uninstalled google-api-core-2.3.2\n",
      "  Attempting uninstall: google-resumable-media\n",
      "    Found existing installation: google-resumable-media 2.1.0\n",
      "    Uninstalling google-resumable-media-2.1.0:\n",
      "      Successfully uninstalled google-resumable-media-2.1.0\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 2.2.1\n",
      "    Uninstalling google-cloud-core-2.2.1:\n",
      "      Successfully uninstalled google-cloud-core-2.2.1\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 2.31.0\n",
      "    Uninstalling google-cloud-bigquery-2.31.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-2.31.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\n",
      "google-cloud-storage 1.43.0 requires google-resumable-media<3.0dev,>=1.3.0; python_version >= \"3.6\", but you have google-resumable-media 0.5.1 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 2.33.0 which is incompatible.\u001b[0m\n",
      "Successfully installed google-api-core-1.31.5 google-auth-1.35.0 google-cloud-bigquery-1.25.0 google-cloud-core-1.7.2 google-resumable-media-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the Google Cloud BigQuery library\n",
    "%pip install google-cloud-bigquery==1.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ignore any incompatibility warnings and errors.\n",
    "**Restart** the kernel to use updated packages. (On the Notebook menu, select Kernel > Restart Kernel > Restart).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.6.2\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary tensorflow library and printing the TF version.\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Tensorflow version: \",tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OS module in python provides functions for interacting with the operating system.\n",
    "import os\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following cell as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change with your own bucket and project below:\n",
    "BUCKET =  \"qwiklabs-gcp-04-26c8364f1a75\"\n",
    "PROJECT = \"qwiklabs-gcp-04-26c8364f1a75\"\n",
    "\n",
    "OUTDIR = \"gs://{bucket}/taxifare/data\".format(bucket=BUCKET)\n",
    "\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['OUTDIR'] = OUTDIR\n",
    "os.environ['PROJECT'] = PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BigQuery tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven not already created a BigQuery dataset for our data, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists\n"
     ]
    }
   ],
   "source": [
    "# Initialize a BigQuery client.\n",
    "bq = bigquery.Client(project = PROJECT)\n",
    "dataset = bigquery.Dataset(bq.dataset(\"taxifare\"))\n",
    "\n",
    "# Create a new dataset with the `create_dataset()` method.\n",
    "try:\n",
    "    bq.create_dataset(dataset)\n",
    "    print(\"Dataset created\")\n",
    "except:\n",
    "    print(\"Dataset already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a table with 1 million examples.\n",
    "\n",
    "Note that the order of columns is exactly what was in our CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: 226cff9e-883b-4808-a841-f5dbcb8819ae\n",
      "Query executing: 0.35s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 404 Not found: Dataset qwiklabs-gcp-04-26c8364f1a75:taxifare was not found in location US\n",
      "\n",
      "(job ID: 226cff9e-883b-4808-a841-f5dbcb8819ae)\n",
      "\n",
      "                        -----Query Job SQL Follows-----                         \n",
      "\n",
      "    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\n",
      "   1:CREATE OR REPLACE TABLE taxifare.feateng_training_data AS\n",
      "   2:\n",
      "   3:SELECT\n",
      "   4:    (tolls_amount + fare_amount) AS fare_amount,\n",
      "   5:    pickup_datetime,\n",
      "   6:    pickup_longitude AS pickuplon,\n",
      "   7:    pickup_latitude AS pickuplat,\n",
      "   8:    dropoff_longitude AS dropofflon,\n",
      "   9:    dropoff_latitude AS dropofflat,\n",
      "  10:    passenger_count*1.0 AS passengers,\n",
      "  11:    'unused' AS key\n",
      "  12:FROM `nyc-tlc.yellow.trips`\n",
      "  13:WHERE ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 1000)) = 1\n",
      "  14:AND\n",
      "  15:    trip_distance > 0\n",
      "  16:    AND fare_amount >= 2.5\n",
      "  17:    AND pickup_longitude > -78\n",
      "  18:    AND pickup_longitude < -70\n",
      "  19:    AND dropoff_longitude > -78\n",
      "  20:    AND dropoff_longitude < -70\n",
      "  21:    AND pickup_latitude > 37\n",
      "  22:    AND pickup_latitude < 45\n",
      "  23:    AND dropoff_latitude > 37\n",
      "  24:    AND dropoff_latitude < 45\n",
      "  25:    AND passenger_count > 0\n",
      "    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\n"
     ]
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_training_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    'unused' AS key\n",
    "FROM `nyc-tlc.yellow.trips`\n",
    "WHERE ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 1000)) = 1\n",
    "AND\n",
    "    trip_distance > 0\n",
    "    AND fare_amount >= 2.5\n",
    "    AND pickup_longitude > -78\n",
    "    AND pickup_longitude < -70\n",
    "    AND dropoff_longitude > -78\n",
    "    AND dropoff_longitude < -70\n",
    "    AND pickup_latitude > 37\n",
    "    AND pickup_latitude < 45\n",
    "    AND dropoff_latitude > 37\n",
    "    AND dropoff_latitude < 45\n",
    "    AND passenger_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the validation dataset be 1/10 the size of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: 19bb6b94-d386-4bca-b90a-4b98bfac377d\n",
      "Query executing: 0.33s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 404 Not found: Dataset qwiklabs-gcp-04-26c8364f1a75:taxifare was not found in location US\n",
      "\n",
      "(job ID: 19bb6b94-d386-4bca-b90a-4b98bfac377d)\n",
      "\n",
      "                         -----Query Job SQL Follows-----                         \n",
      "\n",
      "    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\n",
      "   1:CREATE OR REPLACE TABLE taxifare.feateng_valid_data AS\n",
      "   2:\n",
      "   3:SELECT\n",
      "   4:    (tolls_amount + fare_amount) AS fare_amount,\n",
      "   5:    pickup_datetime,\n",
      "   6:    pickup_longitude AS pickuplon,\n",
      "   7:    pickup_latitude AS pickuplat,\n",
      "   8:    dropoff_longitude AS dropofflon,\n",
      "   9:    dropoff_latitude AS dropofflat,\n",
      "  10:    passenger_count*1.0 AS passengers,\n",
      "  11:    'unused' AS key\n",
      "  12:FROM `nyc-tlc.yellow.trips`\n",
      "  13:WHERE ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 10000)) = 2\n",
      "  14:AND\n",
      "  15:    trip_distance > 0\n",
      "  16:    AND fare_amount >= 2.5\n",
      "  17:    AND pickup_longitude > -78\n",
      "  18:    AND pickup_longitude < -70\n",
      "  19:    AND dropoff_longitude > -78\n",
      "  20:    AND dropoff_longitude < -70\n",
      "  21:    AND pickup_latitude > 37\n",
      "  22:    AND pickup_latitude < 45\n",
      "  23:    AND dropoff_latitude > 37\n",
      "  24:    AND dropoff_latitude < 45\n",
      "  25:    AND passenger_count > 0\n",
      "    |    .    |    .    |    .    |    .    |    .    |    .    |    .    |\n"
     ]
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_valid_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    'unused' AS key\n",
    "FROM `nyc-tlc.yellow.trips`\n",
    "WHERE ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 10000)) = 2\n",
    "AND\n",
    "    trip_distance > 0\n",
    "    AND fare_amount >= 2.5\n",
    "    AND pickup_longitude > -78\n",
    "    AND pickup_longitude < -70\n",
    "    AND dropoff_longitude > -78\n",
    "    AND dropoff_longitude < -70\n",
    "    AND pickup_latitude > 37\n",
    "    AND pickup_latitude < 45\n",
    "    AND dropoff_latitude > 37\n",
    "    AND dropoff_latitude < 45\n",
    "    AND passenger_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the tables as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting current contents of gs://<BUCKET>/taxifare/data\n",
      "Extracting training data to gs://<BUCKET>/taxifare/data\n",
      "BigQuery error in extract operation: Invalid extract destination URI\n",
      "'gs://<BUCKET>/taxifare/data/taxi-train-*.csv'. Must be a valid Google Cloud\n",
      "Storage path and filename/pattern.\n",
      "Extracting validation data to gs://<BUCKET>/taxifare/data\n",
      "BigQuery error in extract operation: Invalid extract destination URI\n",
      "'gs://<BUCKET>/taxifare/data/taxi-valid-*.csv'. Must be a valid Google Cloud\n",
      "Storage path and filename/pattern.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BucketNotFoundException: 404 gs://<BUCKET> bucket does not exist.\n",
      "CommandException: Encountered non-existent bucket during listing\n",
      "BadRequestException: 400 Invalid bucket name: '<BUCKET>'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\necho \"Deleting current contents of $OUTDIR\"\\ngsutil -m -q rm -rf $OUTDIR\\n\\necho \"Extracting training data to $OUTDIR\"\\nbq --location=US extract \\\\\\n   --destination_format CSV  \\\\\\n   --field_delimiter \",\" --noprint_header \\\\\\n   taxifare.feateng_training_data \\\\\\n   $OUTDIR/taxi-train-*.csv\\n\\necho \"Extracting validation data to $OUTDIR\"\\nbq --location=US extract \\\\\\n   --destination_format CSV  \\\\\\n   --field_delimiter \",\" --noprint_header \\\\\\n   taxifare.feateng_valid_data \\\\\\n   $OUTDIR/taxi-valid-*.csv\\n\\n# With `-l` option, gsutil will output additional information about each matching provider, bucket, subdirectory, or object.\\ngsutil ls -l $OUTDIR\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28431/4080434672.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\necho \"Deleting current contents of $OUTDIR\"\\ngsutil -m -q rm -rf $OUTDIR\\n\\necho \"Extracting training data to $OUTDIR\"\\nbq --location=US extract \\\\\\n   --destination_format CSV  \\\\\\n   --field_delimiter \",\" --noprint_header \\\\\\n   taxifare.feateng_training_data \\\\\\n   $OUTDIR/taxi-train-*.csv\\n\\necho \"Extracting validation data to $OUTDIR\"\\nbq --location=US extract \\\\\\n   --destination_format CSV  \\\\\\n   --field_delimiter \",\" --noprint_header \\\\\\n   taxifare.feateng_valid_data \\\\\\n   $OUTDIR/taxi-valid-*.csv\\n\\n# With `-l` option, gsutil will output additional information about each matching provider, bucket, subdirectory, or object.\\ngsutil ls -l $OUTDIR\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\necho \"Deleting current contents of $OUTDIR\"\\ngsutil -m -q rm -rf $OUTDIR\\n\\necho \"Extracting training data to $OUTDIR\"\\nbq --location=US extract \\\\\\n   --destination_format CSV  \\\\\\n   --field_delimiter \",\" --noprint_header \\\\\\n   taxifare.feateng_training_data \\\\\\n   $OUTDIR/taxi-train-*.csv\\n\\necho \"Extracting validation data to $OUTDIR\"\\nbq --location=US extract \\\\\\n   --destination_format CSV  \\\\\\n   --field_delimiter \",\" --noprint_header \\\\\\n   taxifare.feateng_valid_data \\\\\\n   $OUTDIR/taxi-valid-*.csv\\n\\n# With `-l` option, gsutil will output additional information about each matching provider, bucket, subdirectory, or object.\\ngsutil ls -l $OUTDIR\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Deleting current contents of $OUTDIR\"\n",
    "gsutil -m -q rm -rf $OUTDIR\n",
    "\n",
    "echo \"Extracting training data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "   --destination_format CSV  \\\n",
    "   --field_delimiter \",\" --noprint_header \\\n",
    "   taxifare.feateng_training_data \\\n",
    "   $OUTDIR/taxi-train-*.csv\n",
    "\n",
    "echo \"Extracting validation data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "   --destination_format CSV  \\\n",
    "   --field_delimiter \",\" --noprint_header \\\n",
    "   taxifare.feateng_valid_data \\\n",
    "   $OUTDIR/taxi-valid-*.csv\n",
    "\n",
    "# With `-l` option, gsutil will output additional information about each matching provider, bucket, subdirectory, or object.\n",
    "gsutil ls -l $OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cat command outputs the contents of one or more URLs to stdout.\n",
    "!gsutil cat gs://$BUCKET/taxifare/data/taxi-train-000000000000.csv | head -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
