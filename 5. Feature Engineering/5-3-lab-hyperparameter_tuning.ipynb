{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing the Hyperparameter tuning\n",
    "\n",
    "**Learning Objectives**\n",
    "1. Learn how to use `cloudml-hypertune` to report the results for Cloud hyperparameter tuning trial runs\n",
    "2. Learn how to configure the `.yaml` file for submitting a Cloud hyperparameter tuning job\n",
    "3. Submit a hyperparameter tuning job to Cloud AI Platform\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Let's see if we can improve upon that by tuning our hyperparameters.\n",
    "\n",
    "Hyperparameters are parameters that are set *prior* to training a model, as opposed to parameters which are learned *during* training. \n",
    "\n",
    "These include learning rate and batch size, but also model design parameters such as type of activation function and number of hidden units.\n",
    "\n",
    "Here are the four most common ways to finding the ideal hyperparameters:\n",
    "1. Manual\n",
    "2. Grid Search\n",
    "3. Random Search\n",
    "4. Bayesian Optimzation\n",
    "\n",
    "**1. Manual**\n",
    "\n",
    "Traditionally, hyperparameter tuning is a manual trial and error process. A data scientist has some intution about suitable hyperparameters which they use as a starting point, then they observe the result and use that information to try a new set of hyperparameters to try to beat the existing performance. \n",
    "\n",
    "Pros\n",
    "- Educational, builds up your intuition as a data scientist\n",
    "- Inexpensive because only one trial is conducted at a time\n",
    "\n",
    "Cons\n",
    "- Requires alot of time and patience\n",
    "\n",
    "**2. Grid Search**\n",
    "\n",
    "On the other extreme we can use grid search. Define a discrete set of values to try for each hyperparameter then try every possible combination. \n",
    "\n",
    "Pros\n",
    "- Can run hundreds of trials in parallel using the cloud\n",
    "- Gauranteed to find the best solution within the search space\n",
    "\n",
    "Cons\n",
    "- Expensive\n",
    "\n",
    "**3. Random Search**\n",
    "\n",
    "Alternatively define a range for each hyperparamter (e.g. 0-256) and sample uniformly at random from that range. \n",
    "\n",
    "Pros\n",
    "- Can run hundreds of trials in parallel using the cloud\n",
    "- Requires less trials than Grid Search to find a good solution\n",
    "\n",
    "Cons\n",
    "- Expensive (but less so than Grid Search)\n",
    "\n",
    "**4. Bayesian Optimization**\n",
    "\n",
    "Unlike Grid Search and Random Search, Bayesian Optimization takes into account information from  past trials to select parameters for future trials. The details of how this is done is beyond the scope of this notebook, but if you're interested you can read how it works here [here](https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization). \n",
    "\n",
    "Pros\n",
    "- Picks values intelligenty based on results from past trials\n",
    "- Less expensive because requires fewer trials to get a good result\n",
    "\n",
    "Cons\n",
    "- Requires sequential trials for best results, takes longer\n",
    "\n",
    "**AI Platform HyperTune**\n",
    "\n",
    "AI Platform HyperTune, powered by [Google Vizier](https://ai.google/research/pubs/pub46180), uses Bayesian Optimization by default, but [also supports](https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview#search_algorithms) Grid Search and Random Search. \n",
    "\n",
    "\n",
    "When tuning just a few hyperparameters (say less than 4), Grid Search and Random Search work well, but when tunining several hyperparameters and the search space is large Bayesian Optimization is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chown command to change the ownership of the repository\n",
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery==1.25.0 in /home/jupyter/.local/lib/python3.7/site-packages (1.25.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.7.2)\n",
      "Requirement already satisfied: six<2.0.0dev,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.16.0)\n",
      "Requirement already satisfied: google-api-core<2.0dev,>=1.15.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.31.5)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.9.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (1.35.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (3.19.1)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery==1.25.0) (0.5.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (59.6.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (21.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (2.26.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (1.53.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (2021.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (4.2.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (3.0.6)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.9.0->google-cloud-bigquery==1.25.0) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.15.0->google-cloud-bigquery==1.25.0) (3.1)\n"
     ]
    }
   ],
   "source": [
    "# Installing the latest version of the package\n",
    "!pip install --user google-cloud-bigquery==1.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Restart your kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kindly ignore the deprecation warnings and incompatibility errors related to google-cloud-storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary module\n",
    "import os\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change with your own bucket and project below:\n",
    "BUCKET =  \"qwiklabs-gcp-03-9cccebcb8485\"\n",
    "PROJECT = \"qwiklabs-gcp-03-9cccebcb8485\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "OUTDIR = \"gs://{bucket}/taxifare/data\".format(bucket=BUCKET)\n",
    "\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['OUTDIR'] = OUTDIR\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = \"2.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Setting up cloud SDK properties\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make code compatible with AI Platform Training Service\n",
    "In order to make our code compatible with AI Platform Training Service we need to make the following changes:\n",
    "\n",
    "1. Upload data to Google Cloud Storage \n",
    "2. Move code into a trainer Python package\n",
    "4. Submit training job with `gcloud` to train on AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to Google Cloud Storage (GCS)\n",
    "\n",
    "Cloud services don't have access to our local files, so we need to upload them to a location the Cloud servers can read from. In this case we'll use GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BigQuery tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven not already created a BigQuery dataset for our data, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists\n"
     ]
    }
   ],
   "source": [
    "bq = bigquery.Client(project = PROJECT)\n",
    "dataset = bigquery.Dataset(bq.dataset(\"taxifare\"))\n",
    "\n",
    "# Creating a dataset\n",
    "try:\n",
    "    bq.create_dataset(dataset)\n",
    "    print(\"Dataset created\")\n",
    "except:\n",
    "    print(\"Dataset already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a table with 1 million examples.\n",
    "\n",
    "Note that the order of columns is exactly what was in our CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_training_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    'unused' AS key\n",
    "FROM `nyc-tlc.yellow.trips`\n",
    "WHERE ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 1000)) = 1\n",
    "AND\n",
    "    trip_distance > 0\n",
    "    AND fare_amount >= 2.5\n",
    "    AND pickup_longitude > -78\n",
    "    AND pickup_longitude < -70\n",
    "    AND dropoff_longitude > -78\n",
    "    AND dropoff_longitude < -70\n",
    "    AND pickup_latitude > 37\n",
    "    AND pickup_latitude < 45\n",
    "    AND dropoff_latitude > 37\n",
    "    AND dropoff_latitude < 45\n",
    "    AND passenger_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the validation dataset be 1/10 the size of the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "\n",
    "CREATE OR REPLACE TABLE taxifare.feateng_valid_data AS\n",
    "\n",
    "SELECT\n",
    "    (tolls_amount + fare_amount) AS fare_amount,\n",
    "    pickup_datetime,\n",
    "    pickup_longitude AS pickuplon,\n",
    "    pickup_latitude AS pickuplat,\n",
    "    dropoff_longitude AS dropofflon,\n",
    "    dropoff_latitude AS dropofflat,\n",
    "    passenger_count*1.0 AS passengers,\n",
    "    'unused' AS key\n",
    "FROM `nyc-tlc.yellow.trips`\n",
    "WHERE ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 10000)) = 2\n",
    "AND\n",
    "    trip_distance > 0\n",
    "    AND fare_amount >= 2.5\n",
    "    AND pickup_longitude > -78\n",
    "    AND pickup_longitude < -70\n",
    "    AND dropoff_longitude > -78\n",
    "    AND dropoff_longitude < -70\n",
    "    AND pickup_latitude > 37\n",
    "    AND pickup_latitude < 45\n",
    "    AND dropoff_latitude > 37\n",
    "    AND dropoff_latitude < 45\n",
    "    AND passenger_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the tables as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting current contents of gs://qwiklabs-gcp-03-9cccebcb8485/taxifare/data\n",
      "Extracting training data to gs://qwiklabs-gcp-03-9cccebcb8485/taxifare/data\n",
      "Extracting validation data to gs://qwiklabs-gcp-03-9cccebcb8485/taxifare/data\n",
      "  88345235  2021-12-27T09:05:16Z  gs://qwiklabs-gcp-03-9cccebcb8485/taxifare/data/taxi-train-000000000000.csv\n",
      "   8725746  2021-12-27T09:05:27Z  gs://qwiklabs-gcp-03-9cccebcb8485/taxifare/data/taxi-valid-000000000000.csv\n",
      "TOTAL: 2 objects, 97070981 bytes (92.57 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r1c297dcb1f7cc29a_0000017dfb22c743_1 ... (23s) Current status: DONE   \n",
      "Waiting on bqjob_r4041144ef5b1bc46_0000017dfb232aaa_1 ... (1s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Deleting current contents of $OUTDIR\"\n",
    "gsutil -m -q rm -rf $OUTDIR\n",
    "\n",
    "echo \"Extracting training data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "   --destination_format CSV  \\\n",
    "   --field_delimiter \",\" --noprint_header \\\n",
    "   taxifare.feateng_training_data \\\n",
    "   $OUTDIR/taxi-train-*.csv\n",
    "\n",
    "echo \"Extracting validation data to $OUTDIR\"\n",
    "bq --location=US extract \\\n",
    "   --destination_format CSV  \\\n",
    "   --field_delimiter \",\" --noprint_header \\\n",
    "   taxifare.feateng_valid_data \\\n",
    "   $OUTDIR/taxi-valid-*.csv\n",
    "\n",
    "# List the files of the bucket\n",
    "gsutil ls -l $OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.5,2012-11-24 18:06:45 UTC,-73.77674,40.645477,-73.993984,40.69777,2,unused\n",
      "2.9,2010-02-13 23:25:00 UTC,-73.982093,40.740428,-73.978625,40.745023,2,unused\n"
     ]
    }
   ],
   "source": [
    "# Here, it shows the short header for each object\n",
    "!gsutil cat gs://$BUCKET/taxifare/data/taxi-train-000000000000.csv | head -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all ran smoothly, you should be able to list the data bucket by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-03-9cccebcb8485/taxifare/data/taxi-train-000000000000.csv\n",
      "gs://qwiklabs-gcp-03-9cccebcb8485/taxifare/data/taxi-valid-000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "# List the files of the bucket\n",
    "!gsutil ls gs://$BUCKET/taxifare/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move code into python package\n",
    "\n",
    "Here, we moved our code into a python package for training on Cloud AI Platform. Let's just check that the files are there. You should see the following files in the `taxifare/trainer` directory:\n",
    " - `__init__.py`\n",
    " - `model.py`\n",
    " - `task.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 28\n",
      "drwxr-xr-x 3 jupyter jupyter 4096 Dec 27 08:44 .\n",
      "drwxr-xr-x 5 jupyter jupyter 4096 Dec 27 08:56 ..\n",
      "-rw-r--r-- 1 jupyter jupyter    0 Dec 27 08:39 __init__.py\n",
      "drwxr-xr-x 2 jupyter jupyter 4096 Dec 27 08:44 __pycache__\n",
      "-rw-r--r-- 1 jupyter jupyter 8484 Dec 27 08:44 model.py\n",
      "-rw-r--r-- 1 jupyter jupyter 2068 Dec 27 08:44 task.py\n"
     ]
    }
   ],
   "source": [
    "# It will list all the files in the mentioned directory with a long listing format\n",
    "!ls -la taxifare/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use hyperparameter tuning in your training job you must perform the following steps:\n",
    "\n",
    " 1. Specify the hyperparameter tuning configuration for your training job by including a HyperparameterSpec in your TrainingInput object.\n",
    "\n",
    " 2. Include the following code in your training application:\n",
    "\n",
    "  - Parse the command-line arguments representing the hyperparameters you want to tune, and use the values to set the hyperparameters for your training trial.\n",
    "Add your hyperparameter metric to the summary for your graph.\n",
    "\n",
    "  - To submit a hyperparameter tuning job, we must modify `model.py` and `task.py` to expose any variables we want to tune as command line arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./taxifare/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./taxifare/trainer/model.py\n",
    "\n",
    "# Importing the necessary modules\n",
    "import datetime\n",
    "import hypertune\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "from tensorflow import feature_column as fc\n",
    "\n",
    "logging.info(tf.version.VERSION)\n",
    "\n",
    "\n",
    "CSV_COLUMNS = [\n",
    "        'fare_amount',\n",
    "        'pickup_datetime',\n",
    "        'pickup_longitude',\n",
    "        'pickup_latitude',\n",
    "        'dropoff_longitude',\n",
    "        'dropoff_latitude',\n",
    "        'passenger_count',\n",
    "        'key',\n",
    "]\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS = [[0.0], ['na'], [0.0], [0.0], [0.0], [0.0], [0.0], ['na']]\n",
    "DAYS = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "\n",
    "\n",
    "# Splits features and labels from feature dictionary\n",
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in ['key']:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label\n",
    "\n",
    "\n",
    "# Loads dataset using the tf.data API from CSV files\n",
    "def load_dataset(pattern, batch_size, num_repeat):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "        num_epochs=num_repeat,\n",
    "    )\n",
    "    return dataset.map(features_and_labels)\n",
    "\n",
    "\n",
    "# Prefetch overlaps the preprocessing and model execution of a training step\n",
    "def create_train_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=None)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "def create_eval_dataset(pattern, batch_size):\n",
    "    dataset = load_dataset(pattern, batch_size, num_repeat=1)\n",
    "    return dataset.prefetch(1)\n",
    "\n",
    "\n",
    "# Parse a string and return a datetime.datetime\n",
    "def parse_datetime(s):\n",
    "    if type(s) is not str:\n",
    "        s = s.numpy().decode('utf-8')\n",
    "    return datetime.datetime.strptime(s, \"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "\n",
    "# Here, tf.sqrt Computes element-wise square root of the input tensor\n",
    "def euclidean(params):\n",
    "    lon1, lat1, lon2, lat2 = params\n",
    "    londiff = lon2 - lon1\n",
    "    latdiff = lat2 - lat1\n",
    "    return tf.sqrt(londiff*londiff + latdiff*latdiff)\n",
    "\n",
    "\n",
    "# Timestamp.weekday() function return the day of the week represented by the date in the given Timestamp object\n",
    "def get_dayofweek(s):\n",
    "    ts = parse_datetime(s)\n",
    "    return DAYS[ts.weekday()]\n",
    "\n",
    "\n",
    "# It wraps a python function into a TensorFlow op that executes it eagerly\n",
    "@tf.function\n",
    "def dayofweek(ts_in):\n",
    "    return tf.map_fn(\n",
    "        lambda s: tf.py_function(get_dayofweek, inp=[s], Tout=tf.string),\n",
    "        ts_in\n",
    "    )\n",
    "\n",
    "def transform(inputs, NUMERIC_COLS, STRING_COLS, nbuckets):\n",
    "    # Pass-through columns\n",
    "    transformed = inputs.copy()\n",
    "    del transformed['pickup_datetime']\n",
    "\n",
    "    feature_columns = {\n",
    "        colname: fc.numeric_column(colname)\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "\n",
    "    # Scaling longitude from range [-70, -78] to [0, 1]\n",
    "    for lon_col in ['pickup_longitude', 'dropoff_longitude']:\n",
    "        transformed[lon_col] = layers.Lambda(\n",
    "            lambda x: (x + 78)/8.0,\n",
    "            name='scale_{}'.format(lon_col)\n",
    "        )(inputs[lon_col])\n",
    "\n",
    "    # Scaling latitude from range [37, 45] to [0, 1]\n",
    "    for lat_col in ['pickup_latitude', 'dropoff_latitude']:\n",
    "        transformed[lat_col] = layers.Lambda(\n",
    "            lambda x: (x - 37)/8.0,\n",
    "            name='scale_{}'.format(lat_col)\n",
    "        )(inputs[lat_col])\n",
    "\n",
    "    # Adding Euclidean dist (no need to be accurate: NN will calibrate it)\n",
    "    transformed['euclidean'] = layers.Lambda(euclidean, name='euclidean')([\n",
    "        inputs['pickup_longitude'],\n",
    "        inputs['pickup_latitude'],\n",
    "        inputs['dropoff_longitude'],\n",
    "        inputs['dropoff_latitude']\n",
    "    ])\n",
    "    feature_columns['euclidean'] = fc.numeric_column('euclidean')\n",
    "\n",
    "    # hour of day from timestamp of form '2010-02-08 09:17:00+00:00'\n",
    "    transformed['hourofday'] = layers.Lambda(\n",
    "        lambda x: tf.strings.to_number(\n",
    "            tf.strings.substr(x, 11, 2), out_type=tf.dtypes.int32),\n",
    "        name='hourofday'\n",
    "    )(inputs['pickup_datetime'])\n",
    "    feature_columns['hourofday'] = fc.indicator_column(\n",
    "        fc.categorical_column_with_identity(\n",
    "            'hourofday', num_buckets=24))\n",
    "\n",
    "    latbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    lonbuckets = np.linspace(0, 1, nbuckets).tolist()\n",
    "    b_plat = fc.bucketized_column(\n",
    "        feature_columns['pickup_latitude'], latbuckets)\n",
    "    b_dlat = fc.bucketized_column(\n",
    "            feature_columns['dropoff_latitude'], latbuckets)\n",
    "    b_plon = fc.bucketized_column(\n",
    "            feature_columns['pickup_longitude'], lonbuckets)\n",
    "    b_dlon = fc.bucketized_column(\n",
    "            feature_columns['dropoff_longitude'], lonbuckets)\n",
    "    ploc = fc.crossed_column(\n",
    "            [b_plat, b_plon], nbuckets * nbuckets)\n",
    "    dloc = fc.crossed_column(\n",
    "            [b_dlat, b_dlon], nbuckets * nbuckets)\n",
    "    pd_pair = fc.crossed_column([ploc, dloc], nbuckets ** 4)\n",
    "    feature_columns['pickup_and_dropoff'] = fc.embedding_column(\n",
    "            pd_pair, 100)\n",
    "\n",
    "    return transformed, feature_columns\n",
    "\n",
    "\n",
    "# Here, tf.sqrt Computes element-wise square root of the input tensor\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_dnn_model(nbuckets, nnsize, lr):\n",
    "    # input layer is all float except for pickup_datetime which is a string\n",
    "    STRING_COLS = ['pickup_datetime']\n",
    "    NUMERIC_COLS = (\n",
    "            set(CSV_COLUMNS) - set([LABEL_COLUMN, 'key']) - set(STRING_COLS)\n",
    "    )\n",
    "    inputs = {\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='float32')\n",
    "        for colname in NUMERIC_COLS\n",
    "    }\n",
    "    inputs.update({\n",
    "        colname: layers.Input(name=colname, shape=(), dtype='string')\n",
    "        for colname in STRING_COLS\n",
    "    })\n",
    "\n",
    "    # transforms\n",
    "    transformed, feature_columns = transform(\n",
    "        inputs, NUMERIC_COLS, STRING_COLS, nbuckets=nbuckets)\n",
    "    dnn_inputs = layers.DenseFeatures(feature_columns.values())(transformed)\n",
    "\n",
    "    x = dnn_inputs\n",
    "    for layer, nodes in enumerate(nnsize):\n",
    "        x = layers.Dense(nodes, activation='relu', name='h{}'.format(layer))(x)\n",
    "    output = layers.Dense(1, name='fare')(x)\n",
    "    \n",
    "    model = models.Model(inputs, output)\n",
    "    lr_optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=lr_optimizer, loss='mse', metrics=[rmse, 'mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Define train and evaluate method to evaluate performance of the model\n",
    "def train_and_evaluate(hparams):\n",
    "    batch_size = hparams['batch_size']\n",
    "    eval_data_path = hparams['eval_data_path']\n",
    "    nnsize = hparams['nnsize']\n",
    "    nbuckets = hparams['nbuckets']\n",
    "    lr = hparams['lr']\n",
    "    num_evals = hparams['num_evals']\n",
    "    num_examples_to_train_on = hparams['num_examples_to_train_on']\n",
    "    output_dir = hparams['output_dir']\n",
    "    train_data_path = hparams['train_data_path']\n",
    "\n",
    "    if tf.io.gfile.exists(output_dir):\n",
    "        tf.io.gfile.rmtree(output_dir)\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    savedmodel_dir = os.path.join(output_dir, 'savedmodel')\n",
    "    model_export_path = os.path.join(savedmodel_dir, timestamp)\n",
    "    checkpoint_path = os.path.join(output_dir, 'checkpoints')\n",
    "    tensorboard_path = os.path.join(output_dir, 'tensorboard')    \n",
    "\n",
    "    dnn_model = build_dnn_model(nbuckets, nnsize, lr)\n",
    "    logging.info(dnn_model.summary())\n",
    "\n",
    "    trainds = create_train_dataset(train_data_path, batch_size)\n",
    "    evalds = create_eval_dataset(eval_data_path, batch_size)\n",
    "\n",
    "    steps_per_epoch = num_examples_to_train_on // (batch_size * num_evals)\n",
    "\n",
    "    checkpoint_cb = callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                              save_weights_only=True,\n",
    "                                              verbose=1)\n",
    "\n",
    "    tensorboard_cb = callbacks.TensorBoard(tensorboard_path,\n",
    "                                           histogram_freq=1)\n",
    "\n",
    "    history = dnn_model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=max(1, steps_per_epoch),\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[checkpoint_cb, tensorboard_cb]\n",
    "    )\n",
    "\n",
    "    # Exporting the model with default serving function.\n",
    "    tf.saved_model.save(dnn_model, model_export_path)\n",
    "    \n",
    "    # TODO 1\n",
    "    hp_metric = history.history['val_rmse'][num_evals-1]\n",
    "    \n",
    "    # TODO 1\n",
    "    hpt = hypertune.HyperTune()\n",
    "    hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='rmse',\n",
    "        metric_value=hp_metric,\n",
    "        global_step=num_evals\n",
    "    )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting taxifare/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile taxifare/trainer/task.py\n",
    "# Importing the necessary module\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help = \"Batch size for training steps\",\n",
    "        type = int,\n",
    "        default = 32\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help = \"GCS location pattern of eval files\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help = \"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        nargs = \"+\",\n",
    "        type = int,\n",
    "        default=[32, 8]\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--nbuckets\",\n",
    "        help = \"Number of buckets to divide lat and lon with\",\n",
    "        type = int,\n",
    "        default = 10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\",\n",
    "        help = \"learning rate for optimizer\",\n",
    "        type = float,\n",
    "        default = 0.001\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_evals\",\n",
    "        help = \"Number of times to evaluate model on eval data training.\",\n",
    "        type = int,\n",
    "        default = 5\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_examples_to_train_on\",\n",
    "        help = \"Number of examples to train on.\",\n",
    "        type = int,\n",
    "        default = 100\n",
    "    )\n",
    "    parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "        help = \"GCS location to write checkpoints and export models\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help = \"GCS location pattern of train files containing eval URLs\",\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help = \"this model ignores this field, but it is required by gcloud\",\n",
    "        default = \"junk\"\n",
    "    )\n",
    "\n",
    "    args, _  = parser.parse_known_args()\n",
    "        \n",
    "    hparams = args.__dict__\n",
    "    hparams[\"output_dir\"] = os.path.join(\n",
    "        hparams[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "    print(\"output_dir\", hparams[\"output_dir\"])\n",
    "    model.train_and_evaluate(hparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create config.yaml file\n",
    "\n",
    "Specify the hyperparameter tuning configuration for your training job\n",
    "Create a HyperparameterSpec object to hold the hyperparameter tuning configuration for your training job, and add the HyperparameterSpec as the hyperparameters object in your TrainingInput object.\n",
    "\n",
    "In your HyperparameterSpec, set the hyperparameterMetricTag to a value representing your chosen metric. If you don't specify a hyperparameterMetricTag, AI Platform Training looks for a metric with the name training/hptuning/metric. The following example shows how to create a configuration for a metric named metric1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hptuning_config.yaml\n",
    "# Setting parameters for hptuning_config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: BASIC\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 10 # TODO 2\n",
    "    maxParallelTrials: 2 # TODO 2\n",
    "    hyperparameterMetricTag: rmse # TODO 2\n",
    "    enableTrialEarlyStopping: True\n",
    "    params:\n",
    "    - parameterName: lr\n",
    "      # TODO 2\n",
    "      type: DOUBLE\n",
    "      minValue: 0.0001\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: nbuckets\n",
    "      # TODO 2\n",
    "      type: INTEGER\n",
    "      minValue: 10\n",
    "      maxValue: 25\n",
    "      scaleType: UNIT_LINEAR_SCALE\n",
    "    - parameterName: batch_size\n",
    "      # TODO 2\n",
    "      type: DISCRETE\n",
    "      discreteValues:\n",
    "      - 15\n",
    "      - 30\n",
    "      - 50\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report your hyperparameter metric to AI Platform Training\n",
    "\n",
    "The way to report your hyperparameter metric to the AI Platform Training service depends on whether you are using TensorFlow for training or not. It also depends on whether you are using a runtime version or a custom container for training.\n",
    "\n",
    "We recommend that your training code reports your hyperparameter metric to AI Platform Training frequently in order to take advantage of early stopping.\n",
    "\n",
    "TensorFlow with a runtime version\n",
    "If you use an AI Platform Training runtime version and train with TensorFlow, then you can report your hyperparameter metric to AI Platform Training by writing the metric to a TensorFlow summary. Use one of the following functions.\n",
    "\n",
    "You may need to install `cloudml-hypertune` on your machine to run this code locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cloudml-hypertune in /opt/conda/lib/python3.7/site-packages (0.1.0.dev6)\n"
     ]
    }
   ],
   "source": [
    "# Installing the latest version of the package\n",
    "!pip install cloudml-hypertune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kindly ignore, if you get the version warnings related to pip install command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir ./taxifare-model/\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dropoff_latitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropoff_longitude (InputLayer)  [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_longitude (InputLayer)   [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_latitude (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pickup_datetime (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_latitude (Lambda) (None,)              0           dropoff_latitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "scale_dropoff_longitude (Lambda (None,)              0           dropoff_longitude[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "euclidean (Lambda)              (None,)              0           pickup_longitude[0][0]           \n",
      "                                                                 pickup_latitude[0][0]            \n",
      "                                                                 dropoff_longitude[0][0]          \n",
      "                                                                 dropoff_latitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "hourofday (Lambda)              (None,)              0           pickup_datetime[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "passenger_count (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_latitude (Lambda)  (None,)              0           pickup_latitude[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "scale_pickup_longitude (Lambda) (None,)              0           pickup_longitude[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_features (DenseFeatures)  (None, 130)          1000000     scale_dropoff_latitude[0][0]     \n",
      "                                                                 scale_dropoff_longitude[0][0]    \n",
      "                                                                 euclidean[0][0]                  \n",
      "                                                                 hourofday[0][0]                  \n",
      "                                                                 passenger_count[0][0]            \n",
      "                                                                 scale_pickup_latitude[0][0]      \n",
      "                                                                 scale_pickup_longitude[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "h0 (Dense)                      (None, 32)           4192        dense_features[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "h1 (Dense)                      (None, 8)            264         h0[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "fare (Dense)                    (None, 1)            9           h1[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 1,004,465\n",
      "Trainable params: 1,004,465\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "20/20 - 2s - loss: 249.5762 - rmse: 14.0319 - mse: 249.5762 - val_loss: 205.3833 - val_rmse: 13.0276 - val_mse: 205.3833\n",
      "\n",
      "Epoch 00001: saving model to ./taxifare-model/checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 09:05:38.042552: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-12-27 09:05:38.337608: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-27 09:05:38.337664: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-27 09:05:38.337710: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-27 09:05:38.584076: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-12-27 09:05:39.946539: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-12-27 09:05:39.946731: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-12-27 09:05:39.961987: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-12-27 09:05:39.964803: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-12-27 09:05:39.969040: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./taxifare-model/tensorboard/train/plugins/profile/2021_12_27_09_05_39\n",
      "\n",
      "2021-12-27 09:05:39.970749: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./taxifare-model/tensorboard/train/plugins/profile/2021_12_27_09_05_39/tensorflow-2-6-20211227-173355.trace.json.gz\n",
      "2021-12-27 09:05:39.977809: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./taxifare-model/tensorboard/train/plugins/profile/2021_12_27_09_05_39\n",
      "\n",
      "2021-12-27 09:05:39.978295: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./taxifare-model/tensorboard/train/plugins/profile/2021_12_27_09_05_39/tensorflow-2-6-20211227-173355.memory_profile.json.gz\n",
      "2021-12-27 09:05:39.978824: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./taxifare-model/tensorboard/train/plugins/profile/2021_12_27_09_05_39\n",
      "Dumped tool data for xplane.pb to ./taxifare-model/tensorboard/train/plugins/profile/2021_12_27_09_05_39/tensorflow-2-6-20211227-173355.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./taxifare-model/tensorboard/train/plugins/profile/2021_12_27_09_05_39/tensorflow-2-6-20211227-173355.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./taxifare-model/tensorboard/train/plugins/profile/2021_12_27_09_05_39/tensorflow-2-6-20211227-173355.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./taxifare-model/tensorboard/train/plugins/profile/2021_12_27_09_05_39/tensorflow-2-6-20211227-173355.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./taxifare-model/tensorboard/train/plugins/profile/2021_12_27_09_05_39/tensorflow-2-6-20211227-173355.kernel_stats.pb\n",
      "\n",
      "2021-12-27 09:05:42.372408: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Testing our training code locally\n",
    "EVAL_DATA_PATH=./taxifare/tests/data/taxi-valid*\n",
    "TRAIN_DATA_PATH=./taxifare/tests/data/taxi-train*\n",
    "OUTPUT_DIR=./taxifare-model\n",
    "\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "    \n",
    "python3 -m trainer.task \\\n",
    "--eval_data_path $EVAL_DATA_PATH \\\n",
    "--output_dir $OUTPUT_DIR \\\n",
    "--train_data_path $TRAIN_DATA_PATH \\\n",
    "--batch_size 5 \\\n",
    "--num_examples_to_train_on 100 \\\n",
    "--num_evals 1 \\\n",
    "--nbuckets 10 \\\n",
    "--lr 0.001 \\\n",
    "--nnsize 32 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mtrain\u001b[0m/  \u001b[01;34mvalidation\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls taxifare-model/tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below hyperparameter training job step will take **upto 45 minutes** to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-03-9cccebcb8485/taxifare/trained_model_211227_090545 us-central1 taxifare_211227_090545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "API [ml.googleapis.com] not enabled on project [525878299088]. Would you like to\n",
      " enable and retry (this will take a few minutes)? (y/N)?  \n",
      "ERROR: (gcloud.ai-platform.jobs.submit.training) User [525878299088-compute@developer.gserviceaccount.com] does not have permission to access projects instance [qwiklabs-gcp-03-9cccebcb8485] (or it may not exist): AI Platform Training & Prediction API has not been used in project 525878299088 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/ml.googleapis.com/overview?project=525878299088 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\n",
      "- '@type': type.googleapis.com/google.rpc.Help\n",
      "  links:\n",
      "  - description: Google developers console API activation\n",
      "    url: https://console.developers.google.com/apis/api/ml.googleapis.com/overview?project=525878299088\n",
      "- '@type': type.googleapis.com/google.rpc.ErrorInfo\n",
      "  domain: googleapis.com\n",
      "  metadata:\n",
      "    consumer: projects/525878299088\n",
      "    service: ml.googleapis.com\n",
      "  reason: SERVICE_DISABLED\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\nPROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\\nBUCKET=$PROJECT_ID\\nREGION=\"us-central1\"\\nTFVERSION=\"2.4\"\\n\\n# Output directory and jobID\\nOUTDIR=gs://${BUCKET}/taxifare/trained_model_$(date -u +%y%m%d_%H%M%S)\\nJOBID=taxifare_$(date -u +%y%m%d_%H%M%S)\\necho ${OUTDIR} ${REGION} ${JOBID}\\ngsutil -m rm -rf ${OUTDIR}\\n\\n# Model and training hyperparameters\\nBATCH_SIZE=15\\nNUM_EXAMPLES_TO_TRAIN_ON=100\\nNUM_EVALS=10\\nNBUCKETS=10\\nLR=0.001\\nNNSIZE=\"32 8\"\\n\\n# GCS paths\\nGCS_PROJECT_PATH=gs://$BUCKET/taxifare\\nDATA_PATH=$GCS_PROJECT_PATH/data\\nTRAIN_DATA_PATH=$DATA_PATH/taxi-train*\\nEVAL_DATA_PATH=$DATA_PATH/taxi-valid*\\n\\n# TODO 3\\ngcloud ai-platform jobs submit training $JOBID \\\\\\n    --module-name=trainer.task \\\\\\n    --package-path=taxifare/trainer \\\\\\n    --staging-bucket=gs://${BUCKET} \\\\\\n    --config=hptuning_config.yaml \\\\\\n    --python-version=3.7 \\\\\\n    --runtime-version=${TFVERSION} \\\\\\n    --region=${REGION} \\\\\\n    -- \\\\\\n    --eval_data_path $EVAL_DATA_PATH \\\\\\n    --output_dir $OUTDIR \\\\\\n    --train_data_path $TRAIN_DATA_PATH \\\\\\n    --batch_size $BATCH_SIZE \\\\\\n    --num_examples_to_train_on $NUM_EXAMPLES_TO_TRAIN_ON \\\\\\n    --num_evals $NUM_EVALS \\\\\\n    --nbuckets $NBUCKETS \\\\\\n    --lr $LR \\\\\\n    --nnsize $NNSIZE \\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31257/2661039894.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nPROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\\nBUCKET=$PROJECT_ID\\nREGION=\"us-central1\"\\nTFVERSION=\"2.4\"\\n\\n# Output directory and jobID\\nOUTDIR=gs://${BUCKET}/taxifare/trained_model_$(date -u +%y%m%d_%H%M%S)\\nJOBID=taxifare_$(date -u +%y%m%d_%H%M%S)\\necho ${OUTDIR} ${REGION} ${JOBID}\\ngsutil -m rm -rf ${OUTDIR}\\n\\n# Model and training hyperparameters\\nBATCH_SIZE=15\\nNUM_EXAMPLES_TO_TRAIN_ON=100\\nNUM_EVALS=10\\nNBUCKETS=10\\nLR=0.001\\nNNSIZE=\"32 8\"\\n\\n# GCS paths\\nGCS_PROJECT_PATH=gs://$BUCKET/taxifare\\nDATA_PATH=$GCS_PROJECT_PATH/data\\nTRAIN_DATA_PATH=$DATA_PATH/taxi-train*\\nEVAL_DATA_PATH=$DATA_PATH/taxi-valid*\\n\\n# TODO 3\\ngcloud ai-platform jobs submit training $JOBID \\\\\\n    --module-name=trainer.task \\\\\\n    --package-path=taxifare/trainer \\\\\\n    --staging-bucket=gs://${BUCKET} \\\\\\n    --config=hptuning_config.yaml \\\\\\n    --python-version=3.7 \\\\\\n    --runtime-version=${TFVERSION} \\\\\\n    --region=${REGION} \\\\\\n    -- \\\\\\n    --eval_data_path $EVAL_DATA_PATH \\\\\\n    --output_dir $OUTDIR \\\\\\n    --train_data_path $TRAIN_DATA_PATH \\\\\\n    --batch_size $BATCH_SIZE \\\\\\n    --num_examples_to_train_on $NUM_EXAMPLES_TO_TRAIN_ON \\\\\\n    --num_evals $NUM_EVALS \\\\\\n    --nbuckets $NBUCKETS \\\\\\n    --lr $LR \\\\\\n    --nnsize $NNSIZE \\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nPROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\\nBUCKET=$PROJECT_ID\\nREGION=\"us-central1\"\\nTFVERSION=\"2.4\"\\n\\n# Output directory and jobID\\nOUTDIR=gs://${BUCKET}/taxifare/trained_model_$(date -u +%y%m%d_%H%M%S)\\nJOBID=taxifare_$(date -u +%y%m%d_%H%M%S)\\necho ${OUTDIR} ${REGION} ${JOBID}\\ngsutil -m rm -rf ${OUTDIR}\\n\\n# Model and training hyperparameters\\nBATCH_SIZE=15\\nNUM_EXAMPLES_TO_TRAIN_ON=100\\nNUM_EVALS=10\\nNBUCKETS=10\\nLR=0.001\\nNNSIZE=\"32 8\"\\n\\n# GCS paths\\nGCS_PROJECT_PATH=gs://$BUCKET/taxifare\\nDATA_PATH=$GCS_PROJECT_PATH/data\\nTRAIN_DATA_PATH=$DATA_PATH/taxi-train*\\nEVAL_DATA_PATH=$DATA_PATH/taxi-valid*\\n\\n# TODO 3\\ngcloud ai-platform jobs submit training $JOBID \\\\\\n    --module-name=trainer.task \\\\\\n    --package-path=taxifare/trainer \\\\\\n    --staging-bucket=gs://${BUCKET} \\\\\\n    --config=hptuning_config.yaml \\\\\\n    --python-version=3.7 \\\\\\n    --runtime-version=${TFVERSION} \\\\\\n    --region=${REGION} \\\\\\n    -- \\\\\\n    --eval_data_path $EVAL_DATA_PATH \\\\\\n    --output_dir $OUTDIR \\\\\\n    --train_data_path $TRAIN_DATA_PATH \\\\\\n    --batch_size $BATCH_SIZE \\\\\\n    --num_examples_to_train_on $NUM_EXAMPLES_TO_TRAIN_ON \\\\\\n    --num_evals $NUM_EVALS \\\\\\n    --nbuckets $NBUCKETS \\\\\\n    --lr $LR \\\\\\n    --nnsize $NNSIZE \\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "BUCKET=$PROJECT_ID\n",
    "REGION=\"us-central1\"\n",
    "TFVERSION=\"2.4\"\n",
    "\n",
    "# Output directory and jobID\n",
    "OUTDIR=gs://${BUCKET}/taxifare/trained_model_$(date -u +%y%m%d_%H%M%S)\n",
    "JOBID=taxifare_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBID}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "# Model and training hyperparameters\n",
    "BATCH_SIZE=15\n",
    "NUM_EXAMPLES_TO_TRAIN_ON=100\n",
    "NUM_EVALS=10\n",
    "NBUCKETS=10\n",
    "LR=0.001\n",
    "NNSIZE=\"32 8\"\n",
    "\n",
    "# GCS paths\n",
    "GCS_PROJECT_PATH=gs://$BUCKET/taxifare\n",
    "DATA_PATH=$GCS_PROJECT_PATH/data\n",
    "TRAIN_DATA_PATH=$DATA_PATH/taxi-train*\n",
    "EVAL_DATA_PATH=$DATA_PATH/taxi-valid*\n",
    "\n",
    "# TODO 3\n",
    "gcloud ai-platform jobs submit training $JOBID \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=taxifare/trainer \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --config=hptuning_config.yaml \\\n",
    "    --python-version=3.7 \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --region=${REGION} \\\n",
    "    -- \\\n",
    "    --eval_data_path $EVAL_DATA_PATH \\\n",
    "    --output_dir $OUTDIR \\\n",
    "    --train_data_path $TRAIN_DATA_PATH \\\n",
    "    --batch_size $BATCH_SIZE \\\n",
    "    --num_examples_to_train_on $NUM_EXAMPLES_TO_TRAIN_ON \\\n",
    "    --num_evals $NUM_EVALS \\\n",
    "    --nbuckets $NBUCKETS \\\n",
    "    --lr $LR \\\n",
    "    --nnsize $NNSIZE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
